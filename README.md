# LLM Chat Console

Консольное приложение для общения с Claude API (Anthropic) с поддержкой персистентной памяти и автоматического сжатия истории.

## Возможности

- Общение с Claude через консоль
- Персистентная память (SQLite) — история сохраняется между запусками
- Автоматическое сжатие диалога для экономии токенов
- Несколько режимов (персон) ассистента
- Streaming ответов

## Как работает память

1. Все сообщения сохраняются в SQLite (`~/.llm-chat/chat_memory.db`)
2. Каждые 2 сообщения (вопрос + ответ) автоматически сжимаются в краткое summary
3. В запрос отправляется только summary + текущий вопрос — это экономит токены
4. Хранится до 10 последних сообщений; старые удаляются вместе с их summary

## Установка

```bash
# Установить переменные окружения
export ANTHROPIC_API_KEY="your-key"
export OPENROUTER_API_KEY="your-key"  # опционально, для сжатия истории

# Собрать и запустить
./gradlew run
```

## Local LLM Setup (Ollama)

Приложение поддерживает запуск локальной LLM через Ollama как альтернативу Claude API.

### Установка Ollama

#### macOS
```bash
brew install ollama
```

#### Linux
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

#### Windows
Скачайте с [ollama.com/download](https://ollama.com/download)

### Использование

1. Запустите сервис Ollama:
```bash
ollama serve
```

2. Скачайте модель (опционально, автоматически загрузится при первом использовании):
```bash
ollama pull qwen2.5:7b
```

3. Запустите приложение:
```bash
./gradlew run
```

4. Выберите опцию `2) Ollama (local)` при запросе выбора LLM провайдера.

### Конфигурация

- `OLLAMA_HOST` - URL сервера Ollama (по умолчанию: `http://localhost:11434`)
- `ANTHROPIC_API_KEY` - Опционально при использовании Ollama

### Системные требования

- **RAM:** Минимум 8GB (рекомендуется 16GB для лучшей производительности)
- **Диск:** ~5GB для модели qwen2.5:7b
- **CPU:** Многоядерный процессор (GPU не требуется, но ускорит работу)

### Примечания по производительности

Локальная LLM на CPU может генерировать ответы 5-15 секунд. Это нормальное поведение.
Для более быстрых ответов рекомендуется:
- Использовать машину с GPU
- Уменьшить длину промпта
- Использовать меньшую модель (будущая функция)

### Troubleshooting

**"Cannot connect to Ollama"**
- Убедитесь что Ollama запущен: `ollama serve`
- Проверьте доступность порта 11434

**"Model not found"**
- Скачайте модель: `ollama pull qwen2.5:7b`
- Или продолжите работу - модель загрузится автоматически

**Медленные ответы**
- Нормально для CPU-only машин
- Проверьте загрузку системы (Activity Monitor / Task Manager)
- Рассмотрите переключение на Claude API для быстрых ответов

## Команды

| Команда | Описание |
|---------|----------|
| `exit` | Выход из программы |
| `/new`, `/clear` | Начать новый диалог |
| `/stats` | Статистика истории |
| `/changePrompt` | Сменить персону ассистента |
| `/temperature <0.0-1.0>` | Изменить temperature |
| `/maxTokens <число>` | Изменить лимит токенов |
| `/memory show` | Показать последние сообщения |
| `/memory search <текст>` | Поиск в истории |
| `/memory clear` | Очистить всю память |

## Технологии

- Kotlin 2.2 + Coroutines
- Ktor Client (HTTP)
- Exposed (SQLite ORM)
- kotlinx.serialization

## Структура проекта

```
src/main/kotlin/
├── Main.kt                 # Точка входа
├── data/
│   ├── network/            # API клиенты (Anthropic, OpenRouter)
│   ├── persistence/        # SQLite (Exposed)
│   └── repository/         # Репозитории
├── domain/
│   ├── models/             # Модели данных
│   └── usecase/            # Use cases
├── presentation/           # Консольный ввод
└── utils/                  # System prompts
```
